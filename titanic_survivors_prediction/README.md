# Titanic Survivors Prediction

End-to-end supervised classification project to predict Titanic passenger survival (Kaggle-style dataset). The work covers exploratory data analysis, feature engineering, baseline and improved modeling, evaluation, and generation of submission files.

## What this project does

- Data organization
  - Consistent folder layout for raw data, exports, and submissions.
  - Clear separation between training, validation, and test usage.

- Exploratory Data Analysis (EDA)
  - Inspection of missing values, distributions, correlations, and potential data leakage.
  - Visual diagnostics of class imbalance and key feature/target relationships.
  - Iterative refinement from an initial pass (EDA 1.0) to a deeper pass (EDA 2.0).

- Feature Engineering
  - Creation and validation of derived features to capture signal beyond raw columns.
  - Careful handling of numeric vs. categorical features.
  - Scaling of numeric features where appropriate to stabilize model training.

- Baseline Modeling
  - Train/validation split with a fixed random state.
  - A simple baseline classifier (e.g., Logistic Regression) to establish reference metrics.
  - Early, lightweight preprocessing to avoid data leakage.

- Improved Modeling
  - Structured preprocessing (imputation, encoding, scaling) encapsulated together with the estimator.
  - Iterative feature improvements and model selection.
  - Evaluation via validation split and/or cross-validation to estimate generalization performance.

- Producing Results
  - Export of key plots and artifacts for traceability.
  - Creation of submission-ready CSV files for the test set.

## Repository structure (key items)

- dataset/ — Training/test CSVs (not committed)
- exports/ — Figures and artifacts generated by notebooks
- submissions/ — Model predictions ready for submission
- EDA_1.0.ipynb — Initial exploration
- EDA_2.0.ipynb — In-depth exploration and refinements
- feature_engineering.ipynb — Building and validating new features
- model_training_baseline.ipynb — Baseline model and metrics
- model_training_features_improvement.ipynb — Pipeline with feature improvements
- titanic_tutorial.ipynb — Reference notes and experiments
- Datasets_imbalance_classification.png — Class imbalance visualization

## Data

- Standard Titanic train/test files containing demographic and travel-related attributes.
- Target: Survived (0/1).
- Note: Do not commit raw data. Place CSV files under dataset/.

## How to reproduce (project environment)

- Use the project’s configured conda environment (named condavenv) with Python 3.12.x.
- Ensure the usual scientific stack is available (pandas, numpy, scikit-learn, seaborn, matplotlib, jupyter, etc.).
- Place input CSV files under titanic_survivors_prediction/dataset/.
- Open the notebooks and run them in order:
  1) EDA_1.0.ipynb
  2) EDA_2.0.ipynb
  3) feature_engineering.ipynb
  4) model_training_baseline.ipynb
  5) model_training_features_improvement.ipynb
- Outputs:
  - Figures and diagnostics are written to exports/.
  - Submission CSVs are written to submissions/ with the expected columns.

## Methodology in brief

- Clean → Explore → Engineer features → Train baseline → Improve model → Validate → Generate submission.
- Preprocessing includes:
  - Numeric imputation and scaling.
  - Categorical imputation and encoding with safe handling of unseen categories.
- Evaluation focuses on accuracy for comparability, with attention to class imbalance. Consider precision/recall/F1 as needed.

## Results

- Established a reliable baseline using a simple linear model.
- Improved performance by refining features and encapsulating preprocessing with the estimator to prevent leakage.
- Produced submission files aligned with competition requirements.

## Reproducibility

- Fixed random seeds for stochastic components.
- A single, consistent preprocessing-and-model pipeline for both training and inference.
- Environment managed at the project level to ensure consistent package versions.

## Next steps

- Systematic hyperparameter tuning and cross-validation.
- Threshold tuning and calibration to mitigate class imbalance.
- Feature importance analysis (e.g., permutation) and model explainability checks.
- Try additional classifiers and simple ensembles; monitor overfitting with learning curves.

## Acknowledgments

- Dataset: Titanic dataset from the corresponding competition platform.
- Goal: Educational and illustrative purposes for a clean, maintainable ML workflow.

Instructions pour ajouter ce fichier au dépôt

1) Créez un fichier nommé README.md à la racine de titanic_survivors_prediction/ et collez-y le contenu ci-dessus.

2) Ajoutez et validez le fichier avec Git:

```shell script
# Ouvrir un terminal à la racine du repo
cd path/to/learning_ml/titanic_survivors_prediction

# Ajouter et committer
git add README.md
git commit -m "Add README for Titanic Survivors Prediction"
git push
```